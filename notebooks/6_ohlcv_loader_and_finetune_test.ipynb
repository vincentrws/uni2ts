{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHLCV Loader & Fine-Tune Integration Test\n",
    "\n",
    "This notebook verifies the new `OHLCVLoader` (Dataset) and `OHLCVMoiraiModule` integration.\n",
    "\n",
    "Steps:\n",
    "1. Initialize `OHLCVLoader` pointing to real 5m data (Stock 'A').\n",
    "2. Verify data packing (7 channels).\n",
    "3. Initialize `OHLCVMoiraiModule` with custom scaler.\n",
    "4. Run a forward pass to verify normalization logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c15451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from uni2ts.data.ohlcvloader import OHLCVLoader\n",
    "from uni2ts.model.moirai.custom_module import OHLCVMoiraiModule\n",
    "from uni2ts.distribution import StudentTOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712dc04",
   "metadata": {},
   "source": [
    "## 1. Initialize Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30aee813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /opt/uni2ts/data/processed_equities/5m\n",
      "\n",
      "============================================================\n",
      "OHLCVLoader Initialized\n",
      "============================================================\n",
      "  Data path: /opt/uni2ts/data/processed_equities/5m\n",
      "  Window size: 512\n",
      "  Stride: 256\n",
      "  Frequency: 5min\n",
      "  Timezone: America/New_York\n",
      "Indexing windows for 1 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target': tensor([[5.6331e+01, 5.6331e+01, 5.6331e+01,  ..., 1.4651e+05, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [5.6331e+01, 5.6465e+01, 5.5794e+01,  ..., 9.8559e+04, 5.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [5.6241e+01, 5.6241e+01, 5.5302e+01,  ..., 1.0667e+05, 1.0000e+01,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [4.6942e+01, 4.6942e+01, 4.6942e+01,  ..., 0.0000e+00,        nan,\n",
       "          2.0000e+00],\n",
       "         [4.6942e+01, 4.6942e+01, 4.6942e+01,  ..., 0.0000e+00,        nan,\n",
       "          2.0000e+00],\n",
       "         [4.6942e+01, 4.6942e+01, 4.6942e+01,  ..., 0.0000e+00,        nan,\n",
       "          2.0000e+00]]),\n",
       " 'observed_mask': tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]),\n",
       " 'sample_id': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'variate_id': tensor([[0, 1, 2,  ..., 4, 5, 6],\n",
       "         [0, 1, 2,  ..., 4, 5, 6],\n",
       "         [0, 1, 2,  ..., 4, 5, 6],\n",
       "         ...,\n",
       "         [0, 1, 2,  ..., 4, 5, 6],\n",
       "         [0, 1, 2,  ..., 4, 5, 6],\n",
       "         [0, 1, 2,  ..., 4, 5, 6]]),\n",
       " 'prediction_mask': tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to your real data\n",
    "data_path = Path(\"/opt/uni2ts/data/processed_equities/5m\")\n",
    "window_size = 512\n",
    "stride = 256\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "# Initialize loader\n",
    "loader = OHLCVLoader(\n",
    "    data_path=data_path,\n",
    "    window_size=window_size,\n",
    "    stride=stride,\n",
    "    max_stocks=1, # Load just the first one to be quick\n",
    "    freq='5min',\n",
    "    verbose=True\n",
    ")\n",
    "loader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab552280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your real data\n",
    "data_path = Path(\"/opt/uni2ts/data/processed_equities/5m\")\n",
    "window_size = 512\n",
    "stride = 256\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "# Initialize loader\n",
    "loader = OHLCVLoader(\n",
    "    data_path=data_path,\n",
    "    window_size=window_size,\n",
    "    stride=stride,\n",
    "    max_stocks=1, # Load just the first one to be quick\n",
    "    freq='5min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nLoader created with {len(loader)} samples\")\n",
    "\n",
    "# Get one sample\n",
    "sample = loader[0]\n",
    "target = sample['target']\n",
    "variate_id = sample['variate_id']\n",
    "observed_mask = sample['observed_mask']\n",
    "\n",
    "print(\"\\nSample Shapes:\")\n",
    "print(f\"Target (Packed): {target.shape} (Expected: [Time, 7])\")\n",
    "print(f\"Variate ID: {variate_id.shape}\")\n",
    "print(f\"Observed Mask: {observed_mask.shape}\")\n",
    "\n",
    "assert target.shape == (window_size, 7), f\"Expected shape ({window_size}, 7), got {target.shape}\"\n",
    "assert (variate_id[0] == torch.arange(7)).all(), \"Variate IDs should be 0..6\"\n",
    "\n",
    "# Check for masked values (handling of NaNs)\n",
    "has_masked_values = (~observed_mask).any().item()\n",
    "print(f\"Has masked values: {has_masked_values}\")\n",
    "\n",
    "if has_masked_values:\n",
    "     print(\"Correctly identifying missing/NaN values via mask.\")\n",
    "else:\n",
    "     print(\"Warning: No masked values found. Check if data contains gaps/NaNs.\")\n",
    "\n",
    "print(\"\\nLoader Test Passed! Features packed correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test OHLCVMoiraiModule Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Module\n",
    "module = OHLCVMoiraiModule(\n",
    "    distr_output=StudentTOutput(output_domain='real'),\n",
    "    d_model=64,\n",
    "    num_layers=2,\n",
    "    patch_sizes=[16],\n",
    "    max_seq_len=window_size,\n",
    "    attn_dropout_p=0.1,\n",
    "    dropout_p=0.1,\n",
    "    scaling=True\n",
    ")\n",
    "\n",
    "print(\"Module initialized. Scaler type:\", type(module.scaler))\n",
    "print(\"Scaler config:\")\n",
    "print(f\"  Open Index: {module.scaler.open_idx}\")\n",
    "print(f\"  High Index: {module.scaler.high_idx}\")\n",
    "print(f\"  Low Index: {module.scaler.low_idx}\")\n",
    "print(f\"  Volume Index: {module.scaler.volume_idx}\")\n",
    "print(f\"  Minutes Index: {module.scaler.minutes_idx}\")\n",
    "print(f\"  Dow Index: {module.scaler.day_of_week_idx}\")\n",
    "\n",
    "# Prepare batch\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(loader, batch_size=batch_size)\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "# Forward pass\n",
    "# Note: MoiraiModule expects inputs, but we just want to test scaler behavior first\n",
    "# We can access the scaler directly\n",
    "loc, scale = module.scaler(\n",
    "    batch['target'],\n",
    "    batch['observed_mask'],\n",
    "    batch['sample_id'],\n",
    "    batch['variate_id']\n",
    ")\n",
    "\n",
    "print(\"\\nScaler Output Stats:\")\n",
    "print(f\"Loc shape: {loc.shape}\")\n",
    "print(f\"Scale shape: {scale.shape}\")\n",
    "\n",
    "# Verify OHL Grouping (Indices 0, 1, 2 should share stats per window)\n",
    "loc_ohl = loc[:, :, :3] # Batch, Time, 0-2\n",
    "\n",
    "print(\"\\nChecking OHL Grouping:\")\n",
    "diff_oh = (loc[:, :, 0] - loc[:, :, 1]).abs().max().item()\n",
    "diff_ol = (loc[:, :, 0] - loc[:, :, 2]).abs().max().item()\n",
    "print(f\"Max difference between Open and High loc: {diff_oh}\")\n",
    "print(f\"Max difference between Open and Low loc: {diff_ol}\")\n",
    "\n",
    "assert diff_oh < 1e-5, \"Open and High should share normalization stats!\"\n",
    "assert diff_ol < 1e-5, \"Open and Low should share normalization stats!\"\n",
    "\n",
    "print(\"\\nSuccess! Module correctly uses OHLCVPackedScaler with packed data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
